{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e9287000","cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom utils import load_house_data, plot_housing_data_classified, \\\n    animate_logistic_regression, plot_fit_landscape_and_loss\n\nsizes, prices, labels = load_house_data('data/housing_prices.txt')\n\nplot_housing_data_classified(sizes, prices, labels)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"id":"cc1f48f2","cell_type":"code","source":"# Scale the input features (as in the previous exercise)\nsize_scaled = (sizes - np.mean(sizes)) / np.mean(sizes)\nprice_scaled = (prices - np.mean(prices)) / np.mean(prices)\n\n# Create feature list: [size, price, size^2, price^2]\n# (use the scaled features!)\n####### YOUR CODE HERE #######\nfeatures = [\n    size_scaled,           # Feature 0: size\n    price_scaled,          # Feature 1: price  \n    size_scaled**2,      # Feature 2: size^2\n    price_scaled**2      # Feature 3: price^2\n]\n####### END YOUR CODE ########\n\n# Convert to numpy array for easier computation\n# (We also need to transpose the array))\nfeature_matrix = np.array(features).T\n\nfeature_names = ['size', 'price', 'size²', 'price²']\n    ","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"id":"2992418a","cell_type":"code","source":"def sigmoid(z):\n    \"\"\"Sigmoid activation function\"\"\"\n    # To prevent overflow in the exponential function, we clip z\n    z = np.clip(z, -500, 500)\n    ####### YOUR CODE HERE #######\n    return ...\n    ####### END YOUR CODE ########\n\n\ndef calculate_logistic_loss(weights, bias, feature_matrix, labels):\n    \"\"\"Calculate logistic loss for any number of features\"\"\"\n    # Linear combination: z = w1*x1 + w2*x2 + ... + wn*xn + b\n\n    # Calculate the linear combination\n    # Recall: the linear combination is z = w1*x1 + w2*x2 + ... + wn*xn + b\n    # where weights is a vector of shape (n_features,) and features is a matrix of shape (n_samples, n_features).\n    # The bias is a scalar.\n    # we can use np.dot to compute the dot product between features and weights, and then add the bias.\n    # this results in a vector z of shape (n_samples,).\n    z = np.dot(feature_matrix, weights) + bias\n\n    # Apply the sigmoid function to get predictions\n    ##### YOUR CODE HERE #######\n    y_pred = ...\n    ####### END YOUR CODE ########\n    \n    # clip to avoid log(0) issues\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Implement the binary cross-entropy loss\n    # Recall: L = -(1/N) * Σ [y * log(y_pred) + (1 - y) * log(1 - y_pred)]\n    ##### YOUR CODE HERE #######\n    loss = ...\n    ####### END YOUR CODE ########\n    return loss\n\n\ndef gradient_descent_logistic(feature_matrix, labels, learning_rate=0.05, n_iterations=150):\n    \"\"\"Perform gradient descent for logistic regression with any number of features\"\"\"\n    n_features = feature_matrix.shape[1]\n    \n    # Initialize parameters\n    weights = np.zeros(n_features)\n    bias = 0.0\n    \n    # Store training history\n    weights_history = [weights.copy()]\n    bias_history = [bias]\n    loss_history = [calculate_logistic_loss(weights, bias, feature_matrix, labels)]\n    \n    for i in range(n_iterations):\n        # Forward pass\n        # Calculate the linear combination\n        z = np.dot(feature_matrix, weights) + bias\n\n        # Apply the sigmoid function to get predictions\n        ##### YOUR CODE HERE #######\n        y_pred = ...\n        ####### END YOUR CODE ########\n\n        # Calculate gradients\n        error = y_pred - labels\n        weight_gradients = np.dot(feature_matrix.T, error) / len(labels)\n        bias_gradient = np.mean(error)\n        \n        # Update parameters\n        ####### YOUR CODE HERE #######\n        weights -= ...\n        bias -= ...\n        ####### END YOUR CODE ########\n        \n        # Store history\n        weights_history.append(weights.copy())\n        bias_history.append(bias)\n        current_loss = calculate_logistic_loss(weights, bias, feature_matrix, labels)\n        loss_history.append(current_loss)\n    \n    return np.array(weights_history), np.array(bias_history), np.array(loss_history)\n\n\nweights_history, bias_history, loss_history = gradient_descent_logistic(\n    feature_matrix, labels, learning_rate=5.0, n_iterations=150\n)\n        \n# Create animation\nanim = animate_logistic_regression(\n    feature_matrix, labels, weights_history, bias_history, loss_history, feature_names,\n    # save_path=\"output/nonlinear_logistic_regression.mp4\"\n)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())\n","metadata":{"trusted":false},"outputs":[],"execution_count":null}]}